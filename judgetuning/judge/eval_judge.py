import logging
from dataclasses import dataclass
from typing import List

import numpy as np
from scipy.stats import spearmanr

from judgetuning.annotation_dataset import (
    AlpacaEvalDataset,
    ArenaHardDataset,
    AnnotationDataset,
)
from judgetuning.annotation_dataset.judge_tuning_dataset import JudgeTuningDataset
from judgetuning.chatbot_arena_utils import load_chatbot_arena_elo_ratings


@dataclass
class JudgeEvaluationMetrics:
    spearman_correlation: float
    time_per_annotation: float
    cost_per_annotation: float
    time_total: float
    cost_total: float
    n_annotations: int
    n_missing: int
    avg_length: float
    n_empty_completions: int


def eval_judge_annotation_dataset(
    annotation_dataset: AnnotationDataset,
    judge_name: str,
    models: list[str] | None = None,
    instructions_index: list[str] | None = None,
) -> JudgeEvaluationMetrics:
    df_preference = annotation_dataset.df_winrate_against_baseline(
        models=models, instructions=instructions_index, judge_name=judge_name
    )
    if hasattr(annotation_dataset, "judge_completions"):
        judge_completions = annotation_dataset.judge_completions(
            models=models,
            instructions=instructions_index,
            judge_name=judge_name,
        )
    else:
        judge_completions = None

    # compute Spearman correlation with chatbot arena
    parent_dataset = annotation_dataset.name.replace("judge-tuning-", "")
    elo_chatbot_arena = load_chatbot_arena_elo_ratings()
    model_intersection = list(
        set(df_preference.columns).intersection(elo_chatbot_arena.index)
    )
    avg_model_preference = df_preference.mean(axis=0).loc[model_intersection]
    elo_chatbot_arena = elo_chatbot_arena.loc[model_intersection]
    if models is None:
        assert len(avg_model_preference) == len(elo_chatbot_arena)
    spearman_corr = spearmanr(elo_chatbot_arena, avg_model_preference).statistic
    logging.info(
        f"spearman_corr: {spearman_corr} (computed on {len(avg_model_preference)} models)"
    )
    time_per_annotation = annotation_dataset.time_per_annotation(
        models=models, instructions=instructions_index, judge_name=judge_name
    )
    cost_per_annotation = annotation_dataset.cost_per_annotation(
        models=models, instructions=instructions_index, judge_name=judge_name
    )
    n_annotations = annotation_dataset.num_annotations(
        models=models, instructions=instructions_index, judge_name=judge_name
    )
    if judge_completions is not None:
        lengths = [
            len(judgement_completion) if isinstance(judgement_completion, str) else 0
            for instruction_judgements in judge_completions.values
            for judgement_completion in instruction_judgements
        ]
        avg_length = float(np.mean(lengths))
        n_empty_completions = sum([x == 0 for x in lengths])
    else:
        avg_length = None
        n_empty_completions = None

    return JudgeEvaluationMetrics(
        spearman_correlation=float(spearman_corr),
        time_per_annotation=time_per_annotation,
        cost_per_annotation=cost_per_annotation,
        cost_total=cost_per_annotation * n_annotations,
        time_total=time_per_annotation * n_annotations,
        n_annotations=n_annotations,
        n_missing=int(df_preference.isna().sum().sum()),
        avg_length=avg_length,
        n_empty_completions=n_empty_completions,
    )


def eval_judge(
    expid: str,
    judge_name: str,
    annotation_dataset: AnnotationDataset,
    models: list[str] | None = None,
    instructions_index: list[str] | None = None,
) -> JudgeEvaluationMetrics:
    """
    :param judge_name:
    :param models:
    :param instructions_index: instructions considered to compute model performance
    :return: judge metrics obtained by evaluating existing annotations.json files from a judge (can be the ones from
    alpaca-eval or ones generated by calling `genetra_judge_annotations.py`
    """
    # TODO implement prompt filtering
    annotation_dataset = JudgeTuningDataset(
        expid=expid,
        models=models,
        judge_name=judge_name,
        instruction_dataset=annotation_dataset.name,
    )

    models = annotation_dataset.models
    return eval_judge_annotation_dataset(
        annotation_dataset,
        models=models,
        instructions_index=instructions_index,
        judge_name=judge_name,
    )


if __name__ == "__main__":
    # annotation_dataset = ArenaHardDataset()
    annotation_dataset = AlpacaEvalDataset()
    # models = annotation_dataset.chatbot_arena_elo_ratings().index.tolist()
    print(
        eval_judge(
            expid="v2",
            judge_name="v2_meta-llama-Meta-Llama-3.1-8B-Instruct_judge-arena-hard_1_alpaca-eval_4096_4_10_False_True_True_False_True_likert",
            annotation_dataset=annotation_dataset,
            # models=models,
        )
    )
